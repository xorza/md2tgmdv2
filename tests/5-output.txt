To summarize a conversation with an LLM when the full transcript does *not* fit in the token budget, you need a *deterministic compression pipeline*\. The core trick is: *summarize in pieces, then summarize the summaries*, and maintain a *bounded â€œmemoryâ€* you can carry forward\.

Below is a very specific, verbose, implementable approach\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 1\) Decide your hard limits \(donâ€™t guess\)*

You need these numbers before you write code:

1\. *Model context size*
  â¦ `MODEL\_CONTEXT\_TOKENS` \(e\.g\., 8k / 16k / 32k / 128k\)\.

2\. *Reserve room for the modelâ€™s answer*
You canâ€™t use the entire context for input\. Pick:
  â¦ `RESERVED\_OUTPUT\_TOKENS` \(e\.g\., 800â€“2000 depending on how long answers you want\)

3\. *Reserve room for â€œinstructions \+ scaffoldingâ€*
System prompt \+ policies \+ formatting:
  â¦ `RESERVED\_OVERHEAD\_TOKENS` \(e\.g\., 300â€“1200\)

4\. Compute your *maximum safe input budget*:
  â¦ `MAX\_INPUT\_TOKENS \= MODEL\_CONTEXT\_TOKENS \- RESERVED\_OUTPUT\_TOKENS \- RESERVED\_OVERHEAD\_TOKENS`

Example \(16k model\):

â¦ `MODEL\_CONTEXT\_TOKENS \= 16000`
â¦ `RESERVED\_OUTPUT\_TOKENS \= 1500`
â¦ `RESERVED\_OVERHEAD\_TOKENS \= 800`
â¦ `MAX\_INPUT\_TOKENS \= 13700`

Now pick chunk sizes:

â¦ `MAX\_CHUNK\_TOKENS \= 2500â€“4000` \(for each summarization call\)
â¦ `TARGET\_CHUNK\_SUMMARY\_TOKENS \= 250â€“500`
â¦ `TARGET\_GLOBAL\_SUMMARY\_TOKENS \= 600â€“1200`
â¦ `MEMORY\_TOKEN\_LIMIT \= 400â€“800` \(for ongoing chat memory\)

*Important:* implement `token\_count\(text\)` using the model tokenizer \(for OpenAI models, that typically means using a tokenizer library like `tiktoken`\)\. Do not approximate by character count\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 2\) Represent the conversation in a machine\-friendly form*

Store messages as:
```json
\[
  \{"id": 1, "role": "user", "content": "\.\.\.", "timestamp": "\.\.\."\},
  \{"id": 2, "role": "assistant", "content": "\.\.\.", "timestamp": "\.\.\."\},
  \.\.\.
\]
```

Keep stable IDs\. This lets you:

â¦ cite what chunk contains what,
â¦ update summaries later,
â¦ retrieve relevant pieces\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 3\) Step 1: Chunk the conversation \(token\-aware, message boundary safe\)*

You must chunk by message boundaries, not arbitrary splits\.

*âœ¨ Chunking algorithm \(precise logic\)*

â¦ Walk messages from oldest to newest
â¦ Add message to current chunk until adding it would exceed `MAX\_CHUNK\_TOKENS`
â¦ Close the chunk, start a new one

Pseudocode:
```pseudo
function chunk\_messages\(messages, MAX\_CHUNK\_TOKENS\):
    chunks \= \[\]
    current \= \[\]
    current\_tokens \= 0

    for msg in messages:
        msg\_text \= msg\.role \+ ": " \+ msg\.content
        t \= token\_count\(msg\_text\)

        \# Edge case: single message too long
        if t \> MAX\_CHUNK\_TOKENS:
            \# Option A: summarize this message alone \(recommended\)
            chunks\.append\(\[msg\]\)  \# but mark it oversized and handle separately
            continue

        if current\_tokens \+ t \> MAX\_CHUNK\_TOKENS and current not empty:
            chunks\.append\(current\)
            current \= \[msg\]
            current\_tokens \= t
        else:
            current\.append\(msg\)
            current\_tokens \+\= t

    if current not empty:
        chunks\.append\(current\)

    return chunks
```

*âœ¨ Oversized single message handling \(important\)*

If a single message is too large:

â¦ Run a special â€œsummarize this one messageâ€ call
â¦ Replace it with its short summary for downstream chunking

This prevents pipeline failure on huge pasted logs\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 4\) Step 2: Summarize each chunk with a strict schema*

Do *not* ask the model â€œsummarize thisâ€ vaguely\. You want a summary that is useful later, not a nice narrative\.

*âœ¨ Use a structured template \(recommended\)*

*Chunk summary prompt \(copy/paste\)*
>
>You are summarizing a segment of a long userâ€“assistant conversation\.
>
>PURPOSE: Create a compact summary that can replace the raw messages in===>future prompts\.
>
>MUST CAPTURE:
>
>1\. User goals/questions asked in this segment
>2\. Concrete facts and constraints stated \(numbers, deadlines, environment, preferences\)
>3\. Assistantâ€™s substantive outputs \(plans, explanations, decisions, code approachesâ€”describe, donâ€™t paste long code\)
>4\. Decisions/outcomes reached
>5\. Open issues / TODOs created or left unresolved
>
>MUST AVOID:
>
>â¦ greetings, filler, repeated statements
>â¦ non\-persistent details unless they change decisions
>
>OUTPUT FORMAT \(plain text, no JSON, no code fences\):
>
>â¦ Topics:
>â¦ User Goals:
>â¦ Key Facts / Constraints:
>â¦ Assistant Actions / Suggestions:
>â¦ Decisions / Outcomes:
>â¦ Open Questions / TODOs:
>
>HARD LIMIT: <\= 350 tokens\. Be information\-dense\.
>
>\[BEGIN SEGMENT\]
><paste chunk messages here with role labels\>
>\[END SEGMENT\]

For each chunk, you call the LLM and store:
```json
\{
  "chunk\_id": 7,
  "message\_ids": \[120, 121, 122, \.\.\.\],
  "summary": "Topics: \.\.\.\\nUser Goals: \.\.\.\\n\.\.\."
\}
```

This is your *first\-level summary*\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 5\) Step 3: Summarize the summaries \(hierarchical compression\)*

If you have many chunk summaries, even they might not fit\.

So you compress again:

1\. Group chunk summaries into â€œsummary groupsâ€ that fit into `MAX\_CHUNK\_TOKENS`
2\. Summarize each group into a second\-level summary
3\. Repeat until you have 1 global summary

*âœ¨ Grouping logic*

If each chunk summary is \~350 tokens, then a group size of \~8â€“10 often fits \(plus prompt overhead\)\.

Pseudocode:
```pseudo
function group\_summaries\(chunk\_summaries, group\_size\):
    return \[chunk\_summaries\[i:i\+group\_size\] for i in range\(0, n, group\_size\)\]
```

*âœ¨ â€œSummaries of summariesâ€ prompt*
>
>You are compressing multiple summaries of a conversation into a higher\-level summary\.
>
>GOAL: Merge redundancies and preserve only durable, decision\-relevant information:
>
>â¦ stable user preferences/constraints
>â¦ major solutions and conclusions
>â¦ important technical decisions/architectures
>â¦ unresolved issues / TODOs
>
>OUTPUT FORMAT:
>
>â¦ Themes:
>â¦ Timeline \(major phases\):
>â¦ Persistent Facts / Preferences:
>â¦ Key Decisions:
>â¦ Open TODOs:
>
>HARD LIMIT: <\= 450 tokens\.
>
>\[BEGIN INPUT SUMMARIES\]
><paste 8â€“10 chunk summaries in chronological order\>
>\[END INPUT SUMMARIES\]

Repeat until total is within your global target size\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 6\) Step 4: Create a â€œLong\-Term Memoryâ€ object \(bounded, reusable\)*

Once you have a global summary \(or a small number of them\), convert it into a compact memory you can inject into future prompts\.

*âœ¨ Memory\-building prompt*
>
>Convert the following conversation summary into a long\-term memory for future turns\.
>
>Keep only information likely to matter later:
>
>â¦ user profile \(skill level, preferences\)
>â¦ constraints \(tools, deadlines, environment\)
>â¦ ongoing projects and status
>â¦ key decisions and rationale \(brief\)
>â¦ open questions / next steps
>
>Format:
>
>â¦ User Profile:
>â¦ Preferences:
>â¦ Constraints:
>â¦ Projects / Status:
>â¦ Key Decisions:
>â¦ Open Questions / TODOs:
>
>HARD LIMIT: <\= 600 tokens\.
>
>\[BEGIN SUMMARY\]
><global summary here\>
>\[END SUMMARY\]

Store this as `long\_term\_memory`\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 7\) Ongoing conversation: Maintain rolling memory \(so you donâ€™t re\-summarize everything\)*

If this is a live chat system, you want:

â¦ `long\_term\_memory` \(<\= 600 tokens\)
â¦ `recent\_messages` \(raw, last N tokens\)
â¦ optionally `chunk\_summaries` for retrieval

*âœ¨ Update policy \(very specific\)*

Every time a new message arrives:

1\. Append to `recent\_messages`
2\. If `token\_count\(long\_term\_memory \+ recent\_messages \+ system\)` exceeds your budget:
  â¦ select the oldest part of `recent\_messages` \(e\.g\., everything except last 6â€“10 turns\)
  â¦ summarize that segment
  â¦ merge it into `long\_term\_memory` via an â€œupdate memoryâ€ call
  â¦ drop===the summarized raw messages

*ğŸ”¸ Update\-memory prompt \(copy/paste\)*
>
>You maintain a bounded long\-term memory of a userâ€“assistant conversation\.
>
>CURRENT MEMORY:
><existing memory\>
>
>NEW INFORMATION \(summary of older recent turns\):
><segment summary\>
>
>Update the memory:
>
>â¦ Add important persistent info
>â¦ Update/overwrite anything that changed
>â¦ Remove low\-value or obsolete details to stay under size limit
>
>Output the UPDATED MEMORY ONLY in this format:
>
>â¦ User Profile:
>â¦ Preferences:
>â¦ Constraints:
>â¦ Projects / Status:
>â¦ Key Decisions:
>â¦ Open Questions / TODOs:
>
>HARD LIMIT: <\= 600 tokens\.

This keeps you always within budget\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 8\) Retrieval \(optional but highly effective\)*

Instead of only having one memory blob, store:

â¦ chunk summaries \+ embeddings
â¦ group summaries \+ embeddings

At query time:

1\. Embed the userâ€™s new question
2\. Retrieve the top\-K relevant summaries
3\. Put only those in the prompt \(plus long\_term\_memory \+ recent messages\)

This gives better accuracy than a single global summary, without blowing context\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 9\) The â€œfinal prompt assemblyâ€ pattern \(what you actually send to answer\)*

When responding to a new user message, build input like:

1\. *System instructions*
2\. *Long\-term memory* \(bounded\)
3\. *Retrieved relevant summaries* \(top\-K\)
4\. *Recent raw messages* \(short window\)
5\. *Current user request*

This pattern scales indefinitely because you always cap each component\.

â€”â€”â€”â€”â€”â€”â€”â€”

*â­ 10\) Common failure modes \(and how to prevent them\)*

1\. *Summaries become too vague*
  â¦ Fix: enforce a schema and require â€œKey Facts / Constraintsâ€ \+ â€œDecisionsâ€ \+ â€œTODOsâ€\.

2\. *Important numbers/details lost*
  â¦ Fix: explicit instruction: â€œpreserve all numbers, commands, file paths, API namesâ€\.

3\. *Memory grows over time*
  â¦ Fix: strict token limit \+ â€œdrop low\-value detailsâ€ requirement in update\-memory prompt\.

4\. *You summarize too late and overflow*
  â¦ Fix: start compressing at \~60â€“70% of budget, not at 95â€“100%\.

5\. *Model hallucinates missing past context*
  â¦ Fix: in system prompt say: â€œIf not in memory/retrieved context, ask the user or request logs\.â€

â€”â€”â€”â€”â€”â€”â€”â€”

If you tell me:

â¦ which model/context size youâ€™re using,
â¦ whether this is offline summarization or live chat,
â¦ and your language \(Python/JS\),
I can give you a concrete implementation skeleton \(including token counting and the exact â€œwhen to summarizeâ€ thresholds\)\.