You summarize a conversation that doesn’t fit an LLM’s token budget by building a *repeatable compression pipeline* that \(a\) never exceeds the context limit and \(b\) preserves the _right_ information \(goals, constraints, decisions, TODOs\) instead of wasting tokens on narrative\.

The practical solution is a *hierarchical summarization \+ rolling memory* approach:
1\. *Chunk* the transcript into pieces that fit\.
2\. *Summarize each chunk* with a strict schema\.
3\. If still too big, *summarize the summaries* \(repeat until it fits\)\.
4\. Convert the final summary into a *bounded long‑term memory* object\.
5\. For ongoing chat, *incrementally summarize* older turns into memory and keep only a small “recent window” raw\.

Below is a very specific, verbose, implementable guide\.

————————

*⭐ 1\) Establish hard budgets \(non‑negotiable\)*
You must pick explicit numbers based on your model\.

*✨ 1\.1 Context math*
Let:
⦁ `MODEL\_CONTEXT\_TOKENS` \= model max context \(e\.g\., 16,000\)
⦁ `RESERVED\_OUTPUT\_TOKENS` \= how many tokens you want for the model’s reply \(e\.g\., 1,200\)
⦁ `RESERVED\_OVERHEAD\_TOKENS` \= system prompt \+ tool schemas \+ wrappers \(e\.g\., 800\)

Then:
⦁ `MAX\_INPUT\_TOKENS \= MODEL\_CONTEXT\_TOKENS \- RESERVED\_OUTPUT\_TOKENS \- RESERVED\_OVERHEAD\_TOKENS`

Example:
⦁ `MODEL\_CONTEXT\_TOKENS \= 16000`
⦁ `RESERVED\_OUTPUT\_TOKENS \= 1500`
⦁ `RESERVED\_OVERHEAD\_TOKENS \= 800`
⦁ `MAX\_INPUT\_TOKENS \= 13700`

*✨ 1\.2 Summarization budgets*
Choose:
⦁ `MAX\_CHUNK\_TOKENS` \(input to a summarization call\): *2,500–4,000* \(e\.g\., 3,000\)
⦁ `TARGET\_CHUNK\_SUMMARY\_TOKENS`: *250–500* \(e\.g\., 350\)
⦁ `TARGET\_GROUP\_SUMMARY\_TOKENS`: *300–600* \(e\.g\., 450\)
⦁ `TARGET\_GLOBAL\_SUMMARY\_TOKENS`: *600–1,200* \(e\.g\., 900\)

For ongoing chat:
⦁ `MEMORY\_TOKEN\_LIMIT`: *400–800* \(e\.g\., 600\)
⦁ `RECENT\_WINDOW\_TOKEN\_LIMIT`: *2,000–4,000* \(e\.g\., 3,000\)

*✨ 1\.3 Use real token counting*
You need a function like:
⦁ `token\_count\(text\) \-\> int`

Implement it with the model’s tokenizer \(don’t approximate by characters\)\.

————————

*⭐ 2\) Store the conversation in a usable structure*
Your raw data should look like:
```json
\[
  \{"id": 1, "role": "user", "content": "\.\.\.", "ts": "\.\.\."\},
  \{"id": 2, "role": "assistant", "content": "\.\.\.", "ts": "\.\.\."\},
  \.\.\.
\]
```

Why IDs matter:
⦁ You can track “chunk \#12 covers messages 300–341”
⦁ You can re\-summarize only a portion later
⦁ You can debug summary errors

————————

*⭐ 3\) Step A — Chunk the conversation \(token‑aware, message‑boundary safe\)*
You must chunk at message boundaries\.

*✨ 3\.1 Chunking algorithm*```pseudo
function chunk\_messages\(messages, MAX\_CHUNK\_TOKENS\):
    chunks \= \[\]
    current\_chunk \= \[\]
    current\_tokens \= 0

    for msg in messages:
        msg\_text \= msg\.role \+ ": " \+ msg\.content
        t \= token\_count\(msg\_text\)

        if t \> MAX\_CHUNK\_TOKENS:
            \# Oversized single message:
            \# summarize it alone or split it by paragraphs first
            short \= summarize\_single\_message\(msg\)
            replacement \= \{role: "assistant", content: "\(summary of oversized message\) " \+ short\}

            if current\_chunk not empty:
                chunks\.append\(current\_chunk\)
                current\_chunk \= \[\]
                current\_tokens \= 0

            chunks\.append\(\[replacement\]\)
            continue

        if current\_tokens \+ t \> MAX\_CHUNK\_TOKENS and current\_chunk not empty:
            chunks\.append\(current\_chunk\)
            current\_chunk \= \[msg\]
            current\_tokens \= t
        else:
            current\_chunk\.append\(msg\)
            current\_tokens \+\= t

    if current\_chunk not empty:
        chunks\.append\(current\_chunk\)

    return chunks
```

*✨ 3\.2 Oversized single\-message handling \(important\)*
If a single message is huge \(pasted logs, large code===blocks\), you have two safe options:
⦁ *Option 1 \(recommended\):* Run a “summarize this message” call and replace it with the summary\.
⦁ *Option 2:* Split that message into paragraphs/sections, summarize each, then merge\.

If you don’t do this, chunking breaks\.

————————

*⭐ 4\) Step B — Summarize each chunk with a strict schema*
The quality of your whole pipeline depends on the _schema_\. Without it, you’ll lose constraints and decisions\.

*✨ 4\.1 Chunk summarization prompt \(copy‑paste\)*
Use this for every chunk:

>You are summarizing a segment of a long user–assistant conversation\.
>PURPOSE: Produce a compact, information\-dense summary that can replace the raw messages in future prompts\.
>
>MUST CAPTURE \(if present\):
>⦁ User goals/questions/tasks in this segment
>⦁ Key facts and constraints \(numbers, deadlines, environment, versions, file paths, commands\)
>⦁ Assistant’s substantive contributions \(plans, reasoning, designs; describe changes instead of pasting long code\)
>⦁ Decisions/outcomes reached \(what was chosen and why\)
>⦁ Open questions / TODOs created or still unresolved
>
>MUST MINIMIZE: greetings, filler, repetition, incidental detail\.
>
>OUTPUT FORMAT \(plain text, no JSON, no code fences\):
>⦁ Topics:
>⦁ User Goals:
>⦁ Key Facts / Constraints:
>⦁ Assistant Actions / Suggestions:
>⦁ Decisions / Outcomes:
>⦁ Open Questions / TODOs:
>
>HARD LENGTH LIMIT: ≤ 350 tokens\. Preserve all important numbers, commands, filenames, and errors\.
>
>\[BEGIN SEGMENT\]
><paste the chunk messages with "user:" / "assistant:" labels\>
>\[END SEGMENT\]

*✨ 4\.2 Store chunk summaries with metadata*
Store something like:
```json
\{
  "chunk\_id": 7,
  "covers\_message\_ids": \[120,121,122,\.\.\.\],
  "summary": "Topics: \.\.\.\\nUser Goals: \.\.\.\\n\.\.\."
\}
```

Now you have first\-level summaries\.

————————

*⭐ 5\) Step C — Summarize the summaries \(hierarchical compression\)*
If you have many chunk summaries, they may still exceed your final budget\.

*✨ 5\.1 Grouping*
If chunk summaries are \~350 tokens and your `MAX\_CHUNK\_TOKENS` is \~3000, then:
⦁ `group\_size ≈ 8` \(8 × 350 \= 2800 tokens input\) is often safe\.
```pseudo
function group\_items\(items, group\_size\):
    groups \= \[\]
    for i in range\(0, len\(items\), group\_size\):
        groups\.append\(items\[i : i \+ group\_size\]\)
    return groups
```

*✨ 5\.2 Group compression prompt \(copy‑paste\)*
>You are compressing multiple chunk summaries of a conversation into one higher\-level summary\.
>
>GOAL: Merge redundancy while preserving durable, decision\-relevant information:
>⦁ stable user profile/preferences/constraints
>⦁ major solution approaches and why they were chosen
>⦁ key decisions/outcomes
>⦁ long\-running threads and status
>⦁ open questions/TODOs
>
>OUTPUT FORMAT:
>⦁ Themes:
>⦁ Timeline / Phases:
>⦁ Persistent Facts / Preferences:
>⦁ Key Decisions / Outcomes:
>⦁ Open Questions / TODOs:
>
>HARD LENGTH LIMIT: ≤ 450 tokens\. Remove repetition aggressively\.
>
>\[BEGIN INPUT SUMMARIES\]
><paste 8–10 chunk summaries in chronological order\>
>\[END INPUT SUMMARIES\]

Run this per group → you get second\-level summaries\.

*✨ 5\.3 Repeat until it fits*
If second\-level summaries still too large:
⦁ group them again
⦁ summarize again

Stop when you have:
⦁ 1 global summary \(preferred\), or
⦁ a small set that fits your target input\.

This is the “summary tree”:

*raw transcript → chunk summaries → group summaries → global summary*

————————

*⭐ 6\) Step D — Convert global summary into bounded long‑term memory*
Now create a compact memory object designed for future prompt injection\.

*✨ 6\.1 Memory creation prompt \(copy‑paste\)*
>Convert the following conversation summary into long\-term memory for future turns\.
>Keep only information likely to matter later\.
>
>Include:
>⦁ User Profile \(skills, preferences, communication style\)
>⦁ Constraints / Environment===>\(OS, tools, versions, repo structure\)
>⦁ Projects / Status \(what’s being built, current progress\)
>⦁ Key Decisions \+ short rationale
>⦁ Open Questions / TODOs
>
>Output format \(plain text\):
>⦁ User Profile:
>⦁ Preferences:
>⦁ Constraints / Environment:
>⦁ Projects / Status:
>⦁ Key Decisions / Rationale:
>⦁ Open Questions / TODOs:
>
>HARD LIMIT: ≤ 600 tokens\. If you must drop details, drop the least durable ones first\.
>
>\[BEGIN SUMMARY\]
><global summary\>
>\[END SUMMARY\]

Store this as `long\_term\_memory`\.

————————

*⭐ 7\) Ongoing chat: rolling memory updates \(the “never overflow” loop\)*
For a live conversation, maintain:
⦁ `long\_term\_memory` \(≤ 600 tokens\)
⦁ `recent\_messages` \(raw recent window\)
⦁ optional: chunk summaries for retrieval

*✨ 7\.1 Trigger rule \(very explicit\)*
Before every model call, compute:

`prompt\_tokens \= token\_count\(system \+ long\_term\_memory \+ recent\_messages \+ new\_user\_message\)`

If:
⦁ `prompt\_tokens \> MAX\_INPUT\_TOKENS \* 0\.9`

then compress\.

*✨ 7\.2 What to compress*
Keep last `K` turns raw \(e\.g\., 8\)\. Summarize older:
```pseudo
function select\_old\_segment\(recent\_messages, keep\_last\=8, max\_tokens\=2000\):
    if len\(recent\_messages\) <\= keep\_last:
        return \[\]
    candidates \= recent\_messages\[0 : \-keep\_last\]
    return trim\_to\_token\_limit\(candidates, max\_tokens\)
```

*✨ 7\.3 Merge into memory \(update prompt\)*
>You maintain a bounded long\-term memory of a conversation\.
>
>CURRENT MEMORY:
><existing memory\>
>
>NEW INFORMATION \(summary of older recent turns\):
><segment summary\>
>
>Update memory by:
>⦁ adding important persistent info
>⦁ correcting anything that changed
>⦁ removing low\-value/obsolete details to stay under size limit
>
>OUTPUT the UPDATED MEMORY ONLY in this format:
>⦁ User Profile:
>⦁ Preferences:
>⦁ Constraints / Environment:
>⦁ Projects / Status:
>⦁ Key Decisions / Rationale:
>⦁ Open Questions / TODOs:
>
>HARD LIMIT: ≤ 600 tokens\.

Then:
⦁ Replace `long\_term\_memory` with updated version
⦁ Delete those old raw messages from `recent\_messages`

Now you’re safely under budget again\.

————————

*⭐ 8\) Optional but best: retrieval \(semantic search over chunk summaries\)*
A single memory blob can drop details\. The best practical setup is:
1\. Store all chunk summaries\.
2\. Compute embeddings for each chunk summary\.
3\. On each new user request:
  ⦁ embed the request
  ⦁ retrieve top‑K relevant chunk summaries
4\. Build the prompt from:
  ⦁ system instructions
  ⦁ long\_term\_memory
  ⦁ top‑K retrieved summaries
  ⦁ recent raw messages
  ⦁ new user message

This yields higher accuracy without exceeding context\.

————————

*⭐ 9\) Common pitfalls and fixes*
1\. *Summaries get too vague*
Fix: enforce schema and require “Key Facts / Constraints” and “Decisions” and “TODOs”\.
2\. *Numbers / commands / error messages lost*
Fix: explicitly instruct “preserve all numbers, commands, filenames, errors”\.
3\. *Memory grows indefinitely*
Fix: strict `MEMORY\_TOKEN\_LIMIT` and “drop least durable details first”\.
4\. *You overflow anyway*
Fix: start summarizing at 60–70% usage, not at 95–100%\.
5\. *LLM hallucinates missing history*
Fix: system instruction: “If not in memory or retrieved summaries, ask the user for details\.”

————————

If you tell me \(a\) which model/context size you’re using and \(b\) whether you want offline summarization or live rolling memory, I can propose exact default numbers \(chunk size, group size, trigger thresholds\) that typically work well\.