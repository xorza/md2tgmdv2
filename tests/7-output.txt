You do it by turning the LLM into a *multiâ€‘step compressor* and _never_ feeding it more text than fits at once\.

Below is a concrete recipe, written so you can implement it\.

â€”â€”â€”â€”â€”â€”â€”â€”

*âœ 1\. What Youâ€™re Trying To Achieve*

Given:

â¦ A long conversation: a list like  
  `messages \= \[\{role: "user"\|"assistant", content: string\}, â€¦\]`
â¦ An LLM with a fixed context limit \(token budget\), e\.g\. 8k / 16k / 32k / 128k tokens\.

You want to:

1\. Produce a *summary of the whole conversation* that fits in the LLMâ€™s context, and/or
2\. Keep chatting with the user while the LLM still has access to relevant past information, *even after* the raw history no longer fits\.

You cannot just send the whole conversation; instead you:

â¦ *Split* it into chunks that fit;
â¦ *Summarize* each chunk using the LLM;
â¦ Optionally *summarize those summaries* \(hierarchical\);
â¦ Maintain a *small â€œmemoryâ€* object that represents the past\.

â€”â€”â€”â€”â€”â€”â€”â€”

*âœ 2\. Set Explicit Token Budgets*

You must pick explicit numbers, not guess\.

Assume:

â¦ `MODEL\_CONTEXT\_TOKENS` \= max tokens your LLM can accept \(say 16,000\)\.

Define:

â¦ `MAX\_CHUNK\_TOKENS` â€” maximum tokens of _conversation text_ you send in a _single summarization_ call\.  
  Example: `MAX\_CHUNK\_TOKENS \= 3\_000`\.

â¦ `TARGET\_CHUNK\_SUMMARY\_TOKENS` â€” target size of each firstâ€‘level summary\.  
  Example: `300`\.

â¦ `TARGET\_GROUP\_SUMMARY\_TOKENS` â€” target size of each higherâ€‘level summary\.  
  Example: `400`\.

â¦ `TARGET\_GLOBAL\_SUMMARY\_TOKENS` â€” target size of final conversation summary\.  
  Example: `800â€“1\_200`\.

For ongoing chat, also:

â¦ `MEMORY\_TOKEN\_LIMIT` â€” max size of longâ€‘term memory\.  
  Example: `600`\.

â¦ `RECENT\_WINDOW\_TOKEN\_LIMIT` â€” how many tokens of raw â€œrecentâ€ conversation you aim to keep\.  
  Example: `3\_000`\.

Use the modelâ€™s tokenizer \(e\.g\. `tiktoken`\) to write:

```text
token\_count\(text\) \-\> approximate token count
```

Everything else will rely on this\.

â€”â€”â€”â€”â€”â€”â€”â€”

*âœ 3\. Offline: Summarize an Existing Long Conversation*

*ðŸ“š 3\.1\. Step 1 â€“ Chunk the Conversation*

Input:

```text
messages \= \[
  \{role: "user", content: "\.\.\."\},
  \{role: "assistant", content: "\.\.\."\},
  \.\.\.
\]
```

Goal: `chunks\[\]`, where each `chunk` is a list of messages whose total tokens â‰¤ `MAX\_CHUNK\_TOKENS`\.

Pseudocode:

```pseudo
function chunk\_messages\(messages, max\_chunk\_tokens\):
    chunks \= \[\]
    current\_chunk \= \[\]
    current\_tokens \= 0

    for msg in messages:
        text \= msg\.role \+ ": " \+ msg\.content
        t \= token\_count\(text\)

        if t \> max\_chunk\_tokens:
            \# Edge case: single message is longer than allowed\.
            \# Summarize this one separately instead of putting it raw in a chunk\.
            summary \= summarize\_single\_long\_message\_with\_llm\(msg\)
            chunks\.append\(\[
                \{role: "assistant", content: "\(summary of long message\) " \+ summary\}
            \]\)
            continue

        if current\_tokens \+ t \> max\_chunk\_tokens and current\_chunk not empty:
            chunks\.append\(current\_chunk\)
            current\_chunk \= \[msg\]
            current\_tokens \= t
        else:
            current\_chunk\.append\(msg\)
            current\_tokens \+\= t

    if current\_chunk not empty:
        chunks\.append\(current\_chunk\)

    return chunks
```

Notes:

â¦ You always cut at *message boundaries*\.
â¦ `summarize\_single\_long\_message\_with\_llm` is just a oneâ€‘off â€œsummarize this textâ€ call\.

â€”â€”â€”â€”â€”â€”â€”â€”

*ðŸ“š 3\.2\. Step 2 â€“ Summarize Each Chunk with the LLM*

For each chunk, call the LLM with a strict summarization prompt\.

*ðŸ”– Prompt template \(per chunk, copyâ€‘paste\):*



=========

**>You are summarizing a segment of a long conversation between a user and an assistant\.
>
>Your goal is to produce a short but highly informative summary that can replace the raw messages in future prompts\.
>
>*INCLUDE \(only if present and important\):*
>â¦ Main user questions, tasks, and goals in this segment
>â¦ Important facts, constraints, and preferences the user states \(deadlines, environment, skill level, likes/dislikes, etc\.\)
>â¦ Key explanations, designs, solution ideas, and reasoning from the assistant \(described concisely in words; avoid large code blocks\)
>â¦ Any decisions made, final answers given, or conclusions reached
>â¦ Any explicit open questions or TODO items mentioned
>
>*EXCLUDE OR MINIMIZE:*
>â¦ Greetings, small talk, and filler conversation
>â¦ Repetitive text that adds no new information
>â¦ Very lowâ€‘level intermediate steps or details unlikely to matter later
>
>*OUTPUT FORMAT \(plain text, no JSON, no code fences\):*
>â¦ Topics: bullet list of the main topics discussed\.
>â¦ User Goals: bullet list of what the user wanted or asked for in this segment\.
>â¦ Key Facts: bullet list of important facts, constraints, or preferences\.
>â¦ Assistant Actions: bullet list describing what the assistant did \(explained X, proposed Y, wrote code for Z, etc\.\)\.
>â¦ Decisions / Outcomes: bullet list of any conclusions, solutions, or decisions reached\.
>â¦ Open Questions / TODOs: bullet list of unresolved issues or future steps\.
>
>*LENGTH LIMIT:*
>â¦ Maximum 300 tokens\. Be concise and informationâ€‘dense\.
>
>Now summarize the following conversation segment:
>
>\[BEGIN SEGMENT\]  
><insert this chunkâ€™s messages, each prefixed with â€œuser:â€ or â€œassistant:â€\>  
>\[END SEGMENT\]||
