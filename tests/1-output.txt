You do it by turning the LLM into a **multi‚Äëstep compressor** and _never_ feeding it more text than fits at once\.

Below is a concrete recipe, written so you can implement it\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚≠ê 1\. What You‚Äôre Trying To Achieve**

Given:

‚¶Å A long conversation: a list like
`messages \= \[\{role: "user"\|"assistant", content: string\}, ‚Ä¶\]`
‚¶Å An LLM with a fixed context limit \(token budget\), e\.g\. 8k / 16k / 32k / 128k tokens\.

You want to:

‚¶Å Produce a **summary of the whole conversation** that fits in the LLM‚Äôs context, and/or
‚¶Å Keep chatting with the user while the LLM still has access to relevant past information, **even after** the raw history no longer fits\.

You cannot just send the whole conversation; instead you:

‚¶Å **Split** it into chunks that fit;
‚¶Å **Summarize** each chunk using the LLM;
‚¶Å Optionally **summarize those summaries** \(hierarchical\);
‚¶Å Maintain a **small ‚Äúmemory‚Äù** object that represents the past\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚≠ê 2\. Set Explicit Token Budgets**

You must pick explicit numbers, not guess\.

Assume:

‚¶Å `MODEL\_CONTEXT\_TOKENS` \= max tokens your LLM can accept \(say 16,000\)\.

Define:

‚¶Å 
`MAX\_CHUNK\_TOKENS` ‚Äî maximum tokens of _conversation text_ you send in a _single summarization_ call\.
Example: `MAX\_CHUNK\_TOKENS \= 3\_000`\.

‚¶Å 
`TARGET\_CHUNK\_SUMMARY\_TOKENS` ‚Äî target size of each first‚Äëlevel summary\.
Example: `300`\.

‚¶Å 
`TARGET\_GROUP\_SUMMARY\_TOKENS` ‚Äî target size of each higher‚Äëlevel summary\.
Example: `400`\.

‚¶Å 
`TARGET\_GLOBAL\_SUMMARY\_TOKENS` ‚Äî target size of final conversation summary\.
Example: `800‚Äì1\_200`\.

For ongoing chat, also:

‚¶Å 
`MEMORY\_TOKEN\_LIMIT` ‚Äî max size of long‚Äëterm memory\.
Example: `600`\.

‚¶Å 
`RECENT\_WINDOW\_TOKEN\_LIMIT` ‚Äî how many tokens of raw ‚Äúrecent‚Äù conversation you aim to keep\.
Example: `3\_000`\.

Use the model‚Äôs tokenizer \(e\.g\. `tiktoken`\) to write:
```text
token\_count\(text\) \-\> approximate token count
```

Everything else will rely on this\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚≠ê 3\. Offline: Summarize an Existing Long Conversation**

**‚ú® 3\.1\. Step 1 ‚Äì Chunk the Conversation**

Input:
```text
messages \= \[
  \{role: "user", content: "\.\.\."\},
  \{role: "assistant", content: "\.\.\."\},
  \.\.\.
\]
```

Goal: `chunks\[\]`, where each `chunk` is a list of messages whose total tokens ‚â§ `MAX\_CHUNK\_TOKENS`\.

Pseudocode:
```pseudo
function chunk\_messages\(messages, max\_chunk\_tokens\):
    chunks \= \[\]
    current\_chunk \= \[\]
    current\_tokens \= 0

    for msg in messages:
        text \= msg\.role \+ ": " \+ msg\.content
        t \= token\_count\(text\)

        if t \> max\_chunk\_tokens:
            \# Edge case: single message is longer than allowed\.
            \# Summarize this one separately instead of putting it raw in a chunk\.
            summary \= summarize\_single\_long\_message\_with\_llm\(msg\)
            chunks\.append\(\[
                \{role: "assistant", content: "\(summary of long message\) " \+ summary\}
            \]\)
            continue

        if current\_tokens \+ t \> max\_chunk\_tokens and current\_chunk not empty:
            chunks\.append\(current\_chunk\)
            current\_chunk \= \[msg\]
            current\_tokens \= t
        else:
            current\_chunk\.append\(msg\)
            current\_tokens \+\= t

    if current\_chunk not empty:
        chunks\.append\(current\_chunk\)

    return chunks
```

Notes:

‚¶Å You always cut at **message boundaries**\.
‚¶Å `summarize\_single\_long\_message\_with\_llm` is just a one‚Äëoff ‚Äúsummarize this text‚Äù call\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚ú® 3\.2\. Step 2 ‚Äì Summarize Each Chunk with the LLM**

For each chunk, call the LLM with a strict summarization prompt\.

**üî∏ Prompt template \(per chunk, copy‚Äëpaste\):**
>
>You are summarizing a segment of a long conversation between a user and an assistant\.
>
>Your goal is to produce a short but highly informative summary that can replace the raw messages in future prompts\.
>
>**INCLUDE \(only if present and important\):**
>
>‚¶Å Main user questions, tasks, and goals in this segment
>‚¶Å Important facts, constraints, and preferences the user states \(deadlines, environment, skill level, likes/dislikes, etc\.\)
>‚¶Å Key explanations, designs, solution ideas, and reasoning from the assistant \(described concisely in words; avoid large code blocks\)
>‚¶Å Any decisions made, final answers given, or conclusions reached
>‚¶Å Any explicit open questions or TODO items mentioned
>
>**EXCLUDE OR MINIMIZE:**
>
>‚¶Å Greetings, small talk, and filler conversation
>‚¶Å Repetitive text that adds no new information
>‚¶Å Very low‚Äëlevel intermediate steps or details unlikely to matter later
>
>**OUTPUT FORMAT \(plain text, no JSON, no code fences\):**
>
>‚¶Å Topics: bullet list of the main topics discussed\.
>‚¶Å User Goals: bullet list of what the user wanted or asked for in this segment\.
>‚¶Å Key Facts: bullet list of important facts, constraints, or preferences\.
>‚¶Å Assistant Actions: bullet list describing what the assistant did \(explained X, proposed Y, wrote code for Z, etc\.\)\.
>‚¶Å Decisions / Outcomes: bullet list of any conclusions, solutions, or decisions reached\.
>‚¶Å Open Questions / TODOs: bullet list of unresolved issues or future steps\.
>
>**LENGTH LIMIT:**
>
>‚¶Å Maximum 300 tokens\. Be concise and information‚Äëdense\.
>
>Now summarize the following conversation segment:
>
>\[BEGIN SEGMENT\]
><insert this chunk‚Äôs messages, each prefixed with ‚Äúuser:‚Äù or ‚Äúassistant:‚Äù\>
>\[END SEGMENT\]

Code‚Äëish:
```pseudo
chunk\_summaries \= \[\]

for i, chunk in enumerate\(chunks\):
    prompt \= build\_chunk\_summary\_prompt\(chunk\)  \# uses text above
    summary\_text \= call\_llm\(prompt\)
    chunk\_summaries\.append\(\{
        "chunk\_index": i,
        "summary": summary\_text
    \}\)
```

Now the entire long conversation is represented by, say, 50‚Äì200 chunk summaries\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚ú® 3\.3\. Step 3 ‚Äì Summarize the Summaries \(Hierarchy\)**

If all `chunk\_summaries` together are still too big for your use case, compress them again\.

**üî∏ 3\.3\.1\. Group summaries**

Choose `group\_size` so that `group\_size \* TARGET\_CHUNK\_SUMMARY\_TOKENS` fits comfortably under `MAX\_CHUNK\_TOKENS`\.

Example: if chunk summaries ‚âà300 tokens, `group\_size \= 10` ‚áí \~3000 tokens of input\.
```pseudo
group\_size \= 10
groups \= \[\]
for i in range\(0, len\(chunk\_summaries\), group\_size\):
    groups\.append\(chunk\_summaries\[i : i \+ group\_size\]\)
```

Each `group` will be converted into a higher‚Äëlevel summary\.

**üî∏ 3\.3\.2\. Prompt to compress a group of summaries:**
>
>You are compressing multiple conversation summaries into a more compact higher‚Äëlevel summary\.
>
>**INPUT:**
>A list of summaries, in chronological order, each summarizing a segment of a long user‚Äìassistant conversation\.
>
>**GOAL:**
>
>‚¶Å Merge overlapping and redundant information\.
>‚¶Å Preserve important, persistent information about:
>‚¶Å user goals, preferences, constraints, and profile
>‚¶Å key facts and data
>‚¶Å main solution ideas, designs, or strategies
>‚¶Å important decisions and outcomes
>‚¶Å long‚Äërunning tasks or projects and their progress
>‚¶Å important unresolved questions or TODOs
>
>‚¶Å Provide a sense of how the conversation evolved across these segments\.
>
>**OUTPUT FORMAT \(plain text, no JSON\):**
>
>‚¶Å High‚ÄëLevel Themes: bullet list of dominant themes/topics across all segments\.
>‚¶Å Timeline / Phases: numbered list where each item briefly describes a phase \(what the user wanted, what the assistant did, key changes or progress\)\.
>‚¶Å Persistent User Info: bullet list of stable user profile info or preferences seen here\.
>‚¶Å Key Decisions / Facts: bullet list of core facts, designs, or decisions that remain important\.
>‚¶Å Open Issues / TODOs: bullet list of unresolved questions or tasks left after these segments\.
>
>**LENGTH LIMIT:**
>
>‚¶Å Maximum 400 tokens\. Aggressively remove repetition and low‚Äëvalue detail\.
>
>Now compress the following summaries into one higher‚Äëlevel summary:
>
>\[BEGIN SUMMARIES\]
><insert the chunk summaries here, separated and in order\>
>\[END SUMMARIES\]

Pseudocode:
```pseudo
group\_summaries \= \[\]

for group in groups:
    prompt \= build\_multi\_summary\_prompt\(group\)
    summary\_text \= call\_llm\(prompt\)
    group\_summaries\.append\(summary\_text\)
```

If the list `group\_summaries` is still too long in total:

‚¶Å Treat `group\_summaries` as the new ‚Äúchunk\_summaries‚Äù\.
‚¶Å Repeat: group them, compress again\.
‚¶Å Continue until you can fit everything into **one global summary** of ‚â§ `TARGET\_GLOBAL\_SUMMARY\_TOKENS`\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚ú® 3\.4\. Step 4 ‚Äì Convert Global Summary into a Reusable ‚ÄúMemory‚Äù**

Now you have a global textual summary\. Turn it into a small, structured memory object\.

**üî∏ Prompt: global summary ‚Üí long‚Äëterm memory**
>
>You are constructing a compact long‚Äëterm memory from a global summary of a user‚Äìassistant conversation\.
>
>**INPUT:**
>A high‚Äëlevel summary of the entire conversation\.
>
>**TASK:**
>Convert this into a structured memory that contains only information likely to be useful in future conversation turns\.
>
>Focus on:
>
>‚¶Å Who the user is \(skills, preferences, constraints, relevant background\)
>‚¶Å Main projects, tasks, or topics and their status
>‚¶Å Important decisions, conclusions, designs, or facts that should not be forgotten
>‚¶Å Unresolved questions or TODOs that might be revisited later
>
>Ignore:
>
>‚¶Å Transient details, minor side comments, throwaway examples, and small talk
>
>**OUTPUT FORMAT \(plain text, no JSON\):**
>
>‚¶Å User Profile: bullet list of what we know about the user \(skills, preferences, constraints, etc\.\)\.
>‚¶Å Projects / Topics: bullet list of the main projects or topics and their current status\.
>‚¶Å Key Decisions / Facts: bullet list of core facts, designs, or conclusions that matter later\.
>‚¶Å Open Questions / TODOs: bullet list of unresolved issues or planned next steps\.
>
>**LENGTH LIMIT:**
>
>‚¶Å Maximum 600 tokens\. Be very selective and concise\.
>
>Now produce the long‚Äëterm memory from the following summary:
>
>\[BEGIN HIGH‚ÄëLEVEL SUMMARY\]
><insert your final global summary\>
>\[END HIGH‚ÄëLEVEL SUMMARY\]

The model‚Äôs output is your `long\_term\_memory` string: a compressed representation of the entire conversation that you can re‚Äëinject into future prompts\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚≠ê 4\. Online: Keep Summarizing As You Chat**

When conversation keeps growing, you need a **rolling** solution:

‚¶Å Maintain:
‚¶Å a small `long\_term\_memory` \(bounded text\), and
‚¶Å a list of `recent\_messages` \(raw most recent turns\)\.

**‚ú® 4\.1\. State**
```pseudo
state \= \{
    "long\_term\_memory": "",  \# string, ‚â§ MEMORY\_TOKEN\_LIMIT
    "recent\_messages": \[\]    \# list of \{role, content\}
\}
```

**‚ú® 4\.2\. On Every New User Message**

Algorithm:
```pseudo
function handle\_user\_message\(user\_text\):
    \# 1\. Add new user message
    state\.recent\_messages\.append\(\{role: "user", content: user\_text\}\)

    \# 2\. See how big the prompt would be if we include memory \+ all recent messages
    prompt \= build\_answer\_prompt\(
        long\_term\_memory \= state\.long\_term\_memory,
        recent\_messages \= state\.recent\_messages
    \)
    used\_tokens \= token\_count\(prompt\)

    \# 3\. If context is getting large, compress older part of recent\_messages
    if used\_tokens \> MODEL\_CONTEXT\_TOKENS \* 0\.7:
        old\_segment \= select\_old\_segment\(state\.recent\_messages\)
        if old\_segment not empty:
            segment\_summary \= summarize\_segment\(old\_segment\)  \# same chunk\-summary prompt, but on this segment
            \# Remove old\_segment from recent\_messages
            state\.recent\_messages \= state\.recent\_messages \- old\_segment
            \# Merge new summary into long\_term\_memory
            state\.long\_term\_memory \= update\_memory\(
                current\_memory \= state\.long\_term\_memory,
                new\_summary \= segment\_summary
            \)

    \# 4\. Build final prompt using updated memory \+ remaining recent messages
    prompt \= build\_answer\_prompt\(
        long\_term\_memory \= state\.long\_term\_memory,
        recent\_messages \= state\.recent\_messages
    \)

    \# 5\. Ask LLM for the reply
    assistant\_reply \= call\_llm\(prompt\)

    \# 6\. Store reply as part of recent\_messages
    state\.recent\_messages\.append\(\{role: "assistant", content: assistant\_reply\}\)

    return assistant\_reply
```

**‚ú® 4\.3\. Selecting Which Old Messages to Summarize**

Simple policy:

‚¶Å Always keep the last `K` messages raw,
‚¶Å Summarize everything older, up to a token limit\.
```pseudo
function select\_old\_segment\(recent\_messages\):
    keep\_last\_turns \= 8  \# keep last 8 turns uncompressed

    if length\(recent\_messages\) <\= keep\_last\_turns:
        return \[\]

    candidates \= recent\_messages\[0 : \-keep\_last\_turns\]

    \# Now trim candidates to, say, 2000 tokens\.
    old\_segment \= trim\_messages\_to\_token\_limit\(candidates, limit \= 2000\)

    return old\_segment
```

`summarize\_segment\(old\_segment\)` just reuses the chunk summarization prompt, but applied to `old\_segment`\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚ú® 4\.4\. Updating Memory Using the LLM**

You merge `segment\_summary` into existing `long\_term\_memory` with another LLM call\.

**üî∏ Prompt: update memory**
>
>You maintain a long‚Äëterm memory for a user‚Äìassistant conversation\.
>
>CURRENT\_MEMORY:
><insert current long\_term\_memory\>
>
>NEW\_INFORMATION \(summary of a recent conversation segment\):
><insert segment\_summary\>
>
>**TASK:**
>Update CURRENT\_MEMORY so that it:
>
>‚¶Å Adds any new, important long‚Äëterm information from NEW\_INFORMATION\.
>‚¶Å Updates or corrects information that has clearly changed\.
>‚¶Å Optionally removes or shrinks details that are now obsolete or clearly unimportant\.
>
>Focus on:
>
>‚¶Å Persistent user preferences, constraints, and profile info
>‚¶Å On‚Äëgoing projects, tasks, or topics and their updated status
>‚¶Å Important decisions, designs, or conclusions that will matter later
>‚¶Å Open questions or TODOs that might be revisited
>
>Ignore or minimize:
>
>‚¶Å Transient details, one‚Äëoff examples, and minor small talk
>
>**OUTPUT:**
>Return the UPDATED\_MEMORY only, in this structure \(plain text\):
>
>‚¶Å User Profile
>‚¶Å Projects / Topics
>‚¶Å Key Decisions / Facts
>‚¶Å Open Questions / TODOs
>
>**LENGTH LIMIT:**
>
>‚¶Å Maximum 600 tokens\. Compress aggressively and drop low‚Äëimportance details if necessary\.

Implementation:
```pseudo
function update\_memory\(current\_memory, new\_summary\):
    prompt \= build\_update\_memory\_prompt\(current\_memory, new\_summary\)
    updated \= call\_llm\(prompt\)
    return updated
```

This keeps `long\_term\_memory`:

‚¶Å Under `MEMORY\_TOKEN\_LIMIT`, and
‚¶Å Reflecting the whole conversation so far\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚ú® 4\.5\. Building the Actual Prompt for Answers**

To let the LLM answer the user using compressed history:
```pseudo
function build\_answer\_prompt\(long\_term\_memory, recent\_messages\):
    text \= ""
    text \+\= "SYSTEM: You are an AI assistant\. You have access to a summarized long\-term memory of the user and our past conversation\. Use it when relevant, but do not invent details that are not present in the memory or the recent messages\.\\n\\n"

    if long\_term\_memory is not empty:
        text \+\= "\[LONG\-TERM MEMORY\]\\n"
        text \+\= long\_term\_memory \+ "\\n\\n"

    text \+\= "\[RECENT CONVERSATION\]\\n"
    for msg in recent\_messages:
        text \+\= msg\.role \+ ": " \+ msg\.content \+ "\\n"

    return text
```

You send this \(or an equivalent structured version\) as context to the LLM, ensure `token\_count\(text\)` \+ expected answer length ‚â§ `MODEL\_CONTEXT\_TOKENS`, and you‚Äôre good\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

**‚≠ê 5\. Core Rules to Remember**

‚¶Å 
**Never send the full raw conversation once it‚Äôs large\.**
Always send:

‚¶Å A compressed long‚Äëterm memory,
‚¶Å A recent window of raw messages\.

‚¶Å 
**Use the LLM in several passes:**

‚¶Å Chunk‚Äëlevel summarization,
‚¶Å Hierarchical compression \(if needed\),
‚¶Å Memory creation,
‚¶Å Memory updates\.

‚¶Å 
**Prompts must be explicit and structured\.**
Say exactly:

‚¶Å What to keep \(goals, constraints, facts, decisions, open questions\),
‚¶Å What to drop \(small talk, repetition, incidental details\),
‚¶Å How long the output can be\.

‚¶Å 
**Always hard‚Äëcap memory size\.**
Re‚Äëcompress memory when it gets large by feeding it back into an ‚Äúupdate/compress yourself‚Äù prompt\.

‚¶Å 
**Summarize earlier, not at the last second\.**
Start summarizing when context is around 60‚Äì70% full, leaving breathing room\.

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

If you tell me:

‚¶Å Which model you‚Äôre actually using \(and its context length\),
‚¶Å What language you‚Äôre coding in,

I can translate this into near‚Äëready code with real functions and approximate numeric values filled in\.